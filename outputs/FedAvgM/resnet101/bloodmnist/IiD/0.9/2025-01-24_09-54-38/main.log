[2025-01-24 09:54:44,774][flwr][INFO] - Starting Flower simulation, config: num_rounds=5, no round_timeout
[2025-01-24 09:54:50,020][flwr][INFO] - Flower VCE: Ray initialized with resources: {'memory': 7473242112.0, 'object_store_memory': 3736621056.0, 'node:115.145.163.59': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'GPU': 1.0, 'accelerator_type:G': 1.0}
[2025-01-24 09:54:50,021][flwr][INFO] - Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[2025-01-24 09:54:50,021][flwr][INFO] - Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 1}
[2025-01-24 09:54:50,039][flwr][INFO] - Flower VCE: Creating VirtualClientEngineActorPool with 1 actors
[2025-01-24 09:54:50,040][flwr][INFO] - [INIT]
[2025-01-24 09:54:50,040][flwr][INFO] - Requesting initial parameters from one random client
[2025-01-24 09:54:54,713][flwr][INFO] - Received initial parameters from one random client
[2025-01-24 09:54:54,713][flwr][INFO] - Evaluating initial global parameters
[2025-01-24 09:55:20,195][flwr][INFO] - initial parameters (loss, other metrics): 2.098236647721763, {'accuracy': 0.12277111955568547, 'precision': 0.03432354274101041, 'recall': 0.10816993939540114}
[2025-01-24 09:55:20,195][flwr][INFO] - 
[2025-01-24 09:55:20,195][flwr][INFO] - [ROUND 1]
[2025-01-24 09:55:20,195][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 200)
[2025-01-24 09:55:38,078][flwr][INFO] - aggregate_fit: received 5 results and 0 failures
[2025-01-24 09:55:39,000][flwr][WARNING] - No fit_metrics_aggregation_fn provided
[2025-01-24 09:56:04,171][flwr][INFO] - fit progress: (1, 1.9284360575898785, {'accuracy': 0.3428821981876644, 'precision': 0.18253068819307045, 'recall': 0.2583420926120188}, 43.97584064496914)
[2025-01-24 09:56:04,171][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 200)
[2025-01-24 09:56:13,730][flwr][INFO] - aggregate_evaluate: received 10 results and 0 failures
[2025-01-24 09:56:13,730][flwr][INFO] - 
[2025-01-24 09:56:13,730][flwr][INFO] - [ROUND 2]
[2025-01-24 09:56:13,730][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 200)
[2025-01-24 09:56:30,842][flwr][INFO] - aggregate_fit: received 5 results and 0 failures
[2025-01-24 09:56:56,409][flwr][INFO] - fit progress: (2, 2.7438927309535375, {'accuracy': 0.33908213972522655, 'precision': 0.3769903497022481, 'recall': 0.3503617569041639}, 96.21453907497926)
[2025-01-24 09:56:56,410][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 200)
[2025-01-24 09:57:05,494][flwr][INFO] - aggregate_evaluate: received 10 results and 0 failures
[2025-01-24 09:57:05,494][flwr][INFO] - 
[2025-01-24 09:57:05,494][flwr][INFO] - [ROUND 3]
[2025-01-24 09:57:05,494][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 200)
[2025-01-24 09:57:22,504][flwr][INFO] - aggregate_fit: received 5 results and 0 failures
[2025-01-24 09:57:48,500][flwr][INFO] - fit progress: (3, 1.1227473432772628, {'accuracy': 0.6392867582578193, 'precision': 0.5650744571011519, 'recall': 0.5806655255315505}, 148.30546846200014)
[2025-01-24 09:57:48,501][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 200)
[2025-01-24 09:57:57,887][flwr][INFO] - aggregate_evaluate: received 10 results and 0 failures
[2025-01-24 09:57:57,887][flwr][INFO] - 
[2025-01-24 09:57:57,887][flwr][INFO] - [ROUND 4]
[2025-01-24 09:57:57,888][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 200)
[2025-01-24 09:58:17,077][flwr][INFO] - aggregate_fit: received 5 results and 0 failures
[2025-01-24 09:58:45,127][flwr][INFO] - fit progress: (4, 1.1289367397254872, {'accuracy': 0.6197018415667933, 'precision': 0.5613791255046836, 'recall': 0.5422559221615304}, 204.93181246099994)
[2025-01-24 09:58:45,270][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 200)
[2025-01-24 10:00:20,995][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:20,997][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:20,998][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:21,000][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:21,001][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:21,033][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:21,136][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:21,136][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:21,137][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:22,259][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:22,261][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:22,261][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:22,262][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:22,263][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:22,264][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:22,264][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:22,653][flwr][INFO] - aggregate_evaluate: received 2 results and 8 failures
[2025-01-24 10:00:22,653][flwr][INFO] - 
[2025-01-24 10:00:22,653][flwr][INFO] - [ROUND 5]
[2025-01-24 10:00:22,750][flwr][INFO] - configure_fit: strategy sampled 5 clients (out of 200)
[2025-01-24 10:00:22,934][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:23,042][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:23,233][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:23,338][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:23,338][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:00:24,526][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:44,789][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:44,789][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:44,789][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:44,790][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:00:44,790][flwr][INFO] - aggregate_fit: received 0 results and 5 failures
[2025-01-24 10:01:47,007][flwr][INFO] - fit progress: (5, 1.1289367397254872, {'accuracy': 0.6197018415667933, 'precision': 0.5613791255046836, 'recall': 0.5422559221615304}, 386.8116958219907)
[2025-01-24 10:01:47,008][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 200)
[2025-01-24 10:01:47,224][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:47,267][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:47,345][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:47,345][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:47,450][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:47,450][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:47,561][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:47,561][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:47,755][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:47,756][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:47,756][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:47,790][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:48,180][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:48,181][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:48,181][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:48,181][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:48,181][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:48,182][flwr][ERROR] - Traceback (most recent call last):
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 94, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 398, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 279, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/bob/anaconda3/envs/SecurityAnalysisFL/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-01-24 10:01:48,182][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:48,182][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 115.145.163.59, ID: 2c46f7b0b948f5c986d234398c331c77d783ad09954f7049f80f14f4) where the task (actor ID: fe2809586c8be8d4da90925601000000, name=ClientAppActor.__init__, pid=196135, memory used=3.38GB) was running was 14.59GB / 15.35GB (0.950469), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 115.145.163.59`. To see the logs of the worker, use `ray logs worker-aea966c2a334dcfbb7c8aebccf58b95c75cc81495b7fc161076a8f0f*out -ip 115.145.163.59. Top 10 memory users:
PID	MEM(GB)	COMMAND
195315	4.65	python -m src.main hydra.run.dir=outputs/FedAvgM/resnet101/bloodmnist/IiD/0.9/2025-01-24_09-54-38 da...
196135	3.38	ray::ClientAppActor.run
141656	0.32	/usr/share/code/code --type=utility --utility-sub-type=node.mojom.NodeService --lang=en-US --service...
197143	0.29	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
142297	0.25	/usr/share/code/code /home/bob/.vscode/extensions/ms-python.vscode-pylance-2024.12.1/dist/server.bun...
141536	0.21	/usr/share/code/code --type=renderer --crashpad-handler-pid=141315 --enable-crash-reporter=7b3229a0-...
196703	0.21	/opt/google/chrome/chrome
197358	0.19	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
197502	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
196883	0.15	/opt/google/chrome/chrome --type=renderer --string-annotations --crashpad-handler-pid=196712 --enabl...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-01-24 10:01:48,182][flwr][INFO] - aggregate_evaluate: received 0 results and 10 failures
[2025-01-24 10:01:48,182][flwr][INFO] - 
[2025-01-24 10:01:48,183][flwr][INFO] - [SUMMARY]
[2025-01-24 10:01:48,183][flwr][INFO] - Run finished 5 round(s) in 387.99s
[2025-01-24 10:01:48,183][flwr][INFO] - 	History (loss, distributed):
[2025-01-24 10:01:48,183][flwr][INFO] - 		round 1: 1.8930176973342896
[2025-01-24 10:01:48,183][flwr][INFO] - 		round 2: 3.224331796169281
[2025-01-24 10:01:48,183][flwr][INFO] - 		round 3: 1.1482013702392577
[2025-01-24 10:01:48,183][flwr][INFO] - 		round 4: 1.6900051832199097
[2025-01-24 10:01:48,183][flwr][INFO] - 	History (loss, centralized):
[2025-01-24 10:01:48,183][flwr][INFO] - 		round 0: 2.098236647721763
[2025-01-24 10:01:48,183][flwr][INFO] - 		round 1: 1.9284360575898785
[2025-01-24 10:01:48,183][flwr][INFO] - 		round 2: 2.7438927309535375
[2025-01-24 10:01:48,183][flwr][INFO] - 		round 3: 1.1227473432772628
[2025-01-24 10:01:48,184][flwr][INFO] - 		round 4: 1.1289367397254872
[2025-01-24 10:01:48,184][flwr][INFO] - 		round 5: 1.1289367397254872
[2025-01-24 10:01:48,184][flwr][INFO] - 	History (metrics, distributed, evaluate):
[2025-01-24 10:01:48,184][flwr][INFO] - 	{'accuracy': [(1, 0.38333333333333336),
[2025-01-24 10:01:48,184][flwr][INFO] - 	              (2, 0.3333333333333333),
[2025-01-24 10:01:48,184][flwr][INFO] - 	              (3, 0.6333333333333333),
[2025-01-24 10:01:48,184][flwr][INFO] - 	              (4, 0.3333333333333333)]}
[2025-01-24 10:01:48,184][flwr][INFO] - 	History (metrics, centralized):
[2025-01-24 10:01:48,184][flwr][INFO] - 	{'accuracy': [(0, 0.12277111955568547),
[2025-01-24 10:01:48,184][flwr][INFO] - 	              (1, 0.3428821981876644),
[2025-01-24 10:01:48,184][flwr][INFO] - 	              (2, 0.33908213972522655),
[2025-01-24 10:01:48,184][flwr][INFO] - 	              (3, 0.6392867582578193),
[2025-01-24 10:01:48,184][flwr][INFO] - 	              (4, 0.6197018415667933),
[2025-01-24 10:01:48,184][flwr][INFO] - 	              (5, 0.6197018415667933)],
[2025-01-24 10:01:48,184][flwr][INFO] - 	 'precision': [(0, 0.03432354274101041),
[2025-01-24 10:01:48,184][flwr][INFO] - 	               (1, 0.18253068819307045),
[2025-01-24 10:01:48,184][flwr][INFO] - 	               (2, 0.3769903497022481),
[2025-01-24 10:01:48,184][flwr][INFO] - 	               (3, 0.5650744571011519),
[2025-01-24 10:01:48,184][flwr][INFO] - 	               (4, 0.5613791255046836),
[2025-01-24 10:01:48,184][flwr][INFO] - 	               (5, 0.5613791255046836)],
[2025-01-24 10:01:48,184][flwr][INFO] - 	 'recall': [(0, 0.10816993939540114),
[2025-01-24 10:01:48,184][flwr][INFO] - 	            (1, 0.2583420926120188),
[2025-01-24 10:01:48,184][flwr][INFO] - 	            (2, 0.3503617569041639),
[2025-01-24 10:01:48,184][flwr][INFO] - 	            (3, 0.5806655255315505),
[2025-01-24 10:01:48,184][flwr][INFO] - 	            (4, 0.5422559221615304),
[2025-01-24 10:01:48,184][flwr][INFO] - 	            (5, 0.5422559221615304)]}
[2025-01-24 10:01:48,185][flwr][INFO] - 
